install.packages(c('purrr','lubridate'))

library(tidyverse)
library(fixest)
library(ggstance)
library(vtable)
library(multcomp)
library(purrr)
library(lubridate)
library(dplyr)

#Reading in Google Trends Data
trends_files <- list.files(path = '.' , pattern='trends_', full.names=TRUE)

trends_files <- map_df(trends_files, read_csv) 

trends_files

#Aggregating the Google Trends Data: 
"Getting date data"

monthorweek <-str_sub(trends_files$monthorweek, 1,10) 
ymd(monthorweek) 
floor_date(monthorweek, unit= 'month')

"Aggregating"
trends_files %>% 
  group_by(schname, keyword) %>%
  mutate(index_id = (index-mean(index))/sd(index))

"Reading in Scorecard Data"
score <- read_csv('Most+Recent+Cohorts+(Scorecard+Elements).csv')
id_name_link <- read_csv('id_name_link.csv')

'Remove university that share the same schname' 
id_name_link <- id_name_link %>%
                  group_by(schname) %>% 
                  mutate(n= n()) %>% 
                  filter ( n == 1)

'Make column name to lower case to match with id_name_link file'
names(score) <- tolower(names(score))
head(score,10)

'Removing column names that will not be used to reduce file size' 

score <- score %>%
                dplyr::select(unitid, opeid,instnm, city, preddeg, sat_avg, ugds, md_earn_wne_p10-reported-earnings, gt_25k_p6)
  
'Merging the scorecard data'
join_files <- inner_join(id_name_link, trends_files, by='schname')
final_join <- inner_join(join_files, score, by='unitid')

'Save into one file' 
write_csv(final_join, "clean_data.csv")

# Analysis 

"According to a 2020 data retrieved from Northeastern University, the median weekly earnings of someone with a Bachelor's degree
working in the United States is $1,248 with an annual earnings of $64,896." 
"Retrieve from: https://www.northeastern.edu/bachelors-completion/news/average-salary-by-education-level/" 

"With this in mind, I will make $64,896 as a benchmark. Colleges with a median annual earning of $64,896 or higher 
as high-earning colleges and below $64,896 as low-earning colleges." 

#Filter college for bachelor's degree and removing NA values. 
bachelors_degree_trend <- clean_data %>% filter(preddeg == 3, na.rm=TRUE) 

#Creating dummy variables with ifelse() function to set the benchmark, where '1' is high-earning and '0' is low-earning. 
bachelors_degree_trend$medianEarning <- ifelse(bachelors_degree_trend$md_earn_wne_p10-reported-earnings >= 64896, '1', '0') 

colnames(bachelors_degree_trend) 

#Selecting only columns that we want to use in our data set. 

bachelors_degree_trend <- bachelors_degree_trend %>% select(unitid, opeid, instnm, city, preddeg, sat_avg, 
                       ugds, md_earn_wne_p10-reported-earnings, medianEarning, index_std)

# Regression 
"The regression is going to test whether there is an impact on Google Trends index on median earnings as well as sat_avg (SAT average score)."
"There is no good or bad SAT score since different school require different scores. In this case, we will go for the 75th percentile as percentiles can be used
to see how well students did compared to other test-takers. In 2021, the 75th percentile total sat score is 1200-1210. With this, we will assume and set 1200 as a benchmark 
of a good SAT score." 
"Retrieve from: https://www.bestcolleges.com/blog/what-is-a-good-sat-score/" 

clean_data <- clean_data %>%
  mutate(earning = md_earn_wne_p10.reported.earnings >= 64896, SAT = sat_avg >= 1200)


m1 <- feols(index_std ~ earning*SAT, data=clean_data) 
export_summs(m1) 
iplot(m1)


#Visualization 

ggplot(clean_data, aes(x=md_earn_wne_p10-reported-earnings, y=index_std)) + geom_point() + geom_smooth(method= 'lm') + 
      labs(x='Median Earnings', y= index_std', title='..') + geom_vline(aes(md_earn_wne_p10-reported-earnings=64896))

QUESTION: -- should the visualization be the regression?-- 










